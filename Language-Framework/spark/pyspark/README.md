sparkä¸Šæœ‰å¯¹ä¸‰ç§æ•°æ®ç±»å‹çš„æ“ä½œ

* å¯¹äº æ•°æ®åº“è¡¨ï¼šä½¿ç”¨ `spark.sql(sql_str)` è¿›è¡ŒæŸ¥è¯¢, ä¼šå¾—åˆ°ä¸€ä¸ª DataFrame ç»“æœã€‚ç„¶åä½¿ç”¨ DataFrame çš„APIè¿›è¡Œæ“ä½œ
* å¯¹äº DataFrameï¼šæœ‰ä¸€æ•´å¥—å¯¹äº DataFrame æ“ä½œçš„æ–¹æ³•
* å¯¹äº RDDï¼šDataFrame.rdd å¾—åˆ°å°±æ˜¯ rddæ•°æ®è¡¨ç¤ºäº†ï¼Œç„¶åå¯ä»¥ç”¨rddæ“ä½œçš„ä¸€å¥—API

ä¸‰ç§æ•°æ®çš„äº’ç›¸è½¬æ¢ï¼š
* æ•°æ®åº“è¡¨ & DataFrame: 
  * æ•°æ®åº“è¡¨ -> DataFrame: `spark.sql(sql_str)` è¿”å›çš„å°±æ˜¯ `DataFrame`
  * DataFrame -> æ•°æ®åº“è¡¨ï¼š `df.createOrReplaceTempView("view_name")`. æ‰§è¡Œè¯¥æ“ä½œåå°±å¯ä»¥å¯¹ `view_name` è¿›è¡Œ sql æ“ä½œäº†ã€‚
* DataFrame & RDD
  * DataFrame -> RDD: `df.rdd` å¾—åˆ°çš„å°±æ˜¯RDD
  * RDD -> DataFrame: `df = spark.createDataFrame(res_rdd, schema)` å¾—åˆ°çš„å°±æ˜¯ `DataFrame` äº†ï¼› æˆ–è€… `rdd.toDF()` å¯ä»¥å¾—åˆ° `DataFrame`
* æ•°æ®åº“è¡¨ & RDDï¼šä¹‹é—´å¥½åƒæ²¡æœ‰ç›´æ¥è½¬çš„ï¼Œå°±æŠŠ `DataFrame` å½“ä½œæ¡¥æ¢ç”¨å§ã€‚

åŠ¨ä½œæ“ä½œï¼š`count(), show(), take()`


# DataFrame
DataFrameçš„schemaå®šä¹‰äº†DataFrameçš„åˆ—åå’Œç±»å‹
```python
import pyspark.sql.types as T

# StructType å¯ä»¥å¥— StructTypeã€‚ä¼°è®¡ä¸è¡Œ ğŸ™…
schema = T.StructType([
    T.StructField("user_id", T.LongType(), True),
    T.StructField("coupon_id", T.LongType(), True),
    T.StructField("info", T.ArrayType(T.StringType(), True), True)])
```

DataFrameçš„åˆ—ï¼šè¡¨ç¤ºä¸€ä¸ªç®€å•ç±»å‹(æ•´æ•°ï¼Œå­—ç¬¦ä¸² ..)æˆ–è€…ä¸€ä¸ªå¤æ‚ç±»å‹ (Arrayï¼ŒMap)

* `StructType`: ä¸€ä¸ª `StructField` array
* `StructField`: è¡¨ç¤º `DataFrame` æŸåˆ—çš„å­—æ®µæè¿°
* `**Type`: è¡¨ç¤ºæ•°æ®ç±»å‹

å¯¹DataFrameçš„æ“ä½œä¸»è¦æ˜¯ç±»ä¼¼ sql è¯­å¥ä¸­çš„æ“ä½œï¼Œä¸è¿‡æ˜¯éœ€è¦é€šè¿‡ `selectExpr` æ–¹å¼æ¥è°ƒç”¨ã€‚
* å­—æ®µçš„æ“ä½œ
* æ¡ä»¶è¯­å¥ï¼š `df.where("col_name < expr")`
* èšåˆè¯­å¥: `df.groupBy("col_name")` èšåˆè¯­å¥åé¢å¯ä»¥æ¥ç€ä¸€å †èšåˆæ“ä½œã€‚`df.groupBy('col_name1').sum('col_name2')....`
