sparkä¸Šæœ‰å¯¹ä¸‰ç§æ•°æ®ç±»å‹çš„æ“ä½œ

* å¯¹äº æ•°æ®åº“è¡¨ï¼šä½¿ç”¨ `spark.sql(sql_str)` è¿›è¡ŒæŸ¥è¯¢, ä¼šå¾—åˆ°ä¸€ä¸ª DataFrame ç»“æœã€‚ç„¶åä½¿ç”¨ DataFrame çš„APIè¿›è¡Œæ“ä½œ
* å¯¹äº DataFrameï¼šæœ‰ä¸€æ•´å¥—å¯¹äº DataFrame æ“ä½œçš„æ–¹æ³•
* å¯¹äº RDDï¼šDataFrame.rdd å¾—åˆ°å°±æ˜¯ rddæ•°æ®è¡¨ç¤ºäº†ï¼Œç„¶åå¯ä»¥ç”¨rddæ“ä½œçš„ä¸€å¥—API

ä¸‰ç§æ•°æ®çš„äº’ç›¸è½¬æ¢ï¼š
* æ•°æ®åº“è¡¨ & DataFrame: 
  * æ•°æ®åº“è¡¨ -> DataFrame: `spark.sql(sql_str)` è¿”å›çš„å°±æ˜¯ `DataFrame`
  * DataFrame -> æ•°æ®åº“è¡¨ï¼š `df.createOrReplaceTempView("view_name")`. æ‰§è¡Œè¯¥æ“ä½œåå°±å¯ä»¥å¯¹ `view_name` è¿›è¡Œ sql æ“ä½œäº†ã€‚
* DataFrame & RDD
  * DataFrame -> RDD: `df.rdd` å¾—åˆ°çš„å°±æ˜¯RDD
  * RDD -> DataFrame: `df = spark.createDataFrame(res_rdd, schema)` å¾—åˆ°çš„å°±æ˜¯ `DataFrame` äº†ï¼› æˆ–è€… `rdd.toDF()` å¯ä»¥å¾—åˆ° `DataFrame`
* æ•°æ®åº“è¡¨ & RDDï¼šä¹‹é—´å¥½åƒæ²¡æœ‰ç›´æ¥è½¬çš„ï¼Œå°±æŠŠ `DataFrame` å½“ä½œæ¡¥æ¢ç”¨å§ã€‚

åŠ¨ä½œæ“ä½œï¼š`count(), show(), take()`


# DataFrame
DataFrameçš„schemaå®šä¹‰äº†DataFrameçš„åˆ—åå’Œç±»å‹
```python
import pyspark.sql.types as T

# StructType å¯ä»¥å¥— StructTypeã€‚ä¼°è®¡ä¸è¡Œ ğŸ™…
schema = T.StructType([
    T.StructField("user_id", T.LongType(), True),
    T.StructField("coupon_id", T.LongType(), True),
    T.StructField("info", T.ArrayType(T.StringType(), True), True)])
```

DataFrameçš„åˆ—ï¼šè¡¨ç¤ºä¸€ä¸ªç®€å•ç±»å‹(æ•´æ•°ï¼Œå­—ç¬¦ä¸² ..)æˆ–è€…ä¸€ä¸ªå¤æ‚ç±»å‹ (Arrayï¼ŒMap)

* `StructType`: ä¸€ä¸ª `StructField` array
* `StructField`: è¡¨ç¤º `DataFrame` æŸåˆ—çš„å­—æ®µæè¿°
* `**Type`: è¡¨ç¤ºæ•°æ®ç±»å‹

å¯¹DataFrameçš„æ“ä½œä¸»è¦æ˜¯ç±»ä¼¼ sql è¯­å¥ä¸­çš„æ“ä½œï¼Œä¸è¿‡æ˜¯éœ€è¦é€šè¿‡ `selectExpr` æ–¹å¼æ¥è°ƒç”¨ã€‚
* å­—æ®µçš„æ“ä½œ
* æ¡ä»¶è¯­å¥ï¼š `df.where("col_name < expr")`
* èšåˆè¯­å¥: `df.groupBy("col_name")` èšåˆè¯­å¥åé¢å¯ä»¥æ¥ç€ä¸€å †èšåˆæ“ä½œã€‚`df.groupBy('col_name1').sum('col_name2')....`

# RDD
row ç±»å‹ RDDï¼Ÿ éš¾é“è¿˜æœ‰å…¶å®ƒç±»å‹
å¯¹äºrowç±»å‹ï¼Œå¯ä»¥ `row[0]`, å¯ä»¥ `row.col_name`ï¼Œå¯ä»¥ `row['colname']`
Row can be used to create a row object by using named arguments. It is not allowed to omit a named argument to represent that the value is None or missing. This should be explicitly set to None in this case.

```python
from pyspark.sql import Row
row = Row(name="Alice", age=11)
print(row['name'], row['age'])
print(row.name, row.age)
print('name' in row)
print('wrong_key' in row)

Person = Row("name", "age")
print('name' in Person)
print('wrong_key' in Person)
Person("Alice", 11)
```



# çª—å£å‡½æ•°

```sql
aggr_func() over ([partition by ] [order by])
```



```sql
select 
 user_id,
 price,
 sum(price) over (partition by user_id) as user_tot_price
from some_table
```

* `over (partition by )` ç”¨æ¥è¡¨ç¤ºè¡¨ç¤ºçª—å£ã€‚` partition by user_id` æ—¢è¡¨ç¤ºäº†çª—å£åˆ‡åˆ†è§„åˆ™ã€‚ä¹Ÿè¡¨ç¤ºå½“å‰è®°å½•åº”è¯¥å±äºå“ªä¸ªçª—å£
* `sum()` ç”¨äºè¯¥çª—å£çš„èšåˆå‡½æ•°



å¯ç”¨çš„èšåˆå‡½æ•°

```sql
row_number()  over(partition by x order by y) 
rank()  over(partition by x order by y)         -- åˆ†ç›¸åŒçš„æ’åä¸€æ ·ï¼Œä½†æ˜¯åé¢çš„åæ¬¡ä¼šè·³è·ƒ
dense_rank()over(partition by x order by y)     -- åˆ†ç›¸åŒçš„æ’åä¸€æ ·ï¼Œä¸”åé¢åæ¬¡ä¸è·³è·ƒ
first_value() over(partition by x order by y)   -- ç¬¬ä¸€æ¬¡å‡ºç°çš„å€¼èµ‹å€¼ç»™ æœ¬çª—å£å†…çš„æ‰€æœ‰è®°å½•
last_value() over(partition by x order by y)    -- æœ€åä¸€æ¬¡å‡ºç°çš„å€¼èµ‹å€¼ç»™æœ¬çª—å£çš„æ‰€æœ‰è®°å½•
sum(*) over(partition by x order by y)          -- åˆ†ç»„å†…æ±‡æ€»å€¼ åŠ order byåˆ™æ˜¯ç»„å†…æˆªæ­¢å½“å‰æ’åºæ±‡æ€»å€¼ï¼ˆç­‰åŒäºrows between unbounded preceding and current rowï¼‰ï¼Œä¸åŠ æ’åºåˆ™æ˜¯åˆ†ç»„å†…æ±‡æ€»å€¼

count(*) over(partition by x order by y)        -- åˆ†ç»„å†…è®°å½•å€¼åŠ order byåˆ™æ˜¯ç»„å†…æˆªæ­¢å½“å‰æ’åºè®°å½•å€¼ï¼ˆç­‰åŒäºrows between unbounded preceding and current rowï¼‰ï¼Œä¸åŠ æ’åºåˆ™æ˜¯åˆ†ç»„å†…æ€»è®°å½•

cume_dist() over(partition by x order by y)     -- è¿”å›å°äºç­‰äºå½“å‰å€¼çš„è¡Œæ•°/åˆ†ç»„å†…æ€»è¡Œæ•°,éœ€åŠ order byï¼Œä¸åŠ æ²¡æ„ä¹‰
min(*) over(partition by x order by y)          -- åˆ†ç»„å†…æœ€å°å€¼åŠ order byåˆ™æ˜¯ç»„å†…æˆªæ­¢å½“å‰æ’åºæœ€å°ï¼ˆç­‰åŒäºrows between unbounded preceding and current rowï¼‰ï¼Œä¸åŠ æ’åºåˆ™æ˜¯åˆ†ç»„å†…æœ€å°å€¼

max(*) over(partition by x order by y)
percent_rank() over(partition by x order by y)    -- è®¡ç®—ç»™å®šè¡Œçš„ç™¾åˆ†æ¯”æ’åã€‚å¯ä»¥ç”¨æ¥è®¡ç®—è¶…è¿‡äº†ç™¾åˆ†ä¹‹å¤šå°‘çš„äººã€‚å¦‚360å°åŠ©æ‰‹å¼€æœºé€Ÿåº¦è¶…è¿‡äº†ç™¾åˆ†ä¹‹å¤šå°‘çš„äºº. (å½“å‰è¡Œçš„rankå€¼-1)/(åˆ†ç»„å†…çš„æ€»è¡Œæ•°-1)


lag(col,n,DEFAULT) over(partition by x order by y) -- çª—å£å†…å½“å‰è¡Œå¾€å‰æ•°n è¡Œçš„å€¼ã€‚
lead(col,n,DEFAULT) over(partition by x order by y) -- çª—å£å†…å½“å‰è¡Œå¾€åæ•°n è¡Œçš„å€¼ã€‚
NTILE(n) OVER(partition by x order by y)            -- åˆ†ç‰‡ã€‚å°†çª—å£å†…æ•°æ®æŒ‰ç…§é¡ºåºåˆ‡æˆ n ç‰‡ã€‚å¹¶è¿”å›å½“å‰è¡Œæ‰€åœ¨çš„åˆ†ç‰‡æ•° 
```



# cube & rollup & grouping sets

> group by å¸¸ç”¨å·¥å…·



```sql
select 
	dt,
	first_cat,
	second_cat,
	count(*) as num
from some_table
group by dt, first_cat, second_cat with cube

-- ç­‰ä»·äº åˆ†åˆ« group by dt, first_cat, second_cat ä»»æ„ç»„åˆ(2^n ä¸ª)ã€‚ç„¶å union all èµ·æ¥
```

```sql
select 
	dt,
	first_cat,
	second_cat,
	count(*) as num
from some_table
group by dt, first_cat, second_cat with rollup

-- ç­‰ä»·äº åˆ†åˆ« group by dt, 
--            group by dt, first_cat, 
--            group by dt, first_cat, second_cat. ç„¶å union all èµ·æ¥ï¼ï¼
```



```sql
select 
	dt,
	first_cat,
	second_cat,
	count(*) as num
from some_table
group by dt, first_cat, second_cat
	grouping sets((dt, first_cat), (first_cat, second_cat), (second_cat))
-- å¯¹æŒ‡å®šçš„è¿›è¡Œ group byï¼Œç„¶å union all èµ·æ¥
```

