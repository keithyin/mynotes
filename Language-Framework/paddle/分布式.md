# paddle 分布式

**分布式计算**

* 首先 **一份代码** 会被拷贝到多台机器上去, 不同角色的机器会执行代码的不同部分(`pserver, worker`)
* 在`paddle`分布式中, **程序运行时的环境变量**决定了当前程序的角色是 `pserver` 还是 `worker`

```python
"""
这些变量有:
1. 共有环境变量
export PADDLE_TRAINERS_NUM=2 # 训练节点数
export PADDLE_PSERVERS_IP_PORT_LIST="127.0.0.1:36011,127.0.0.1:36012" # 各个pserver的ip:port 组合构成的字符串

2. pserver 特有环境变量
export TRAINING_ROLE=PSERVER # 当前节点的角色是PSERVER
export PADDLE_PORT=36011 # 当前PSERVER的通信端口
export POD_IP=127.0.0.1 # 当前PSERVER的ip地址

3. Trainer 特有环境变量
export TRAINING_ROLE=TRAINER # 当前节点的角色是TRAINER
export PADDLE_TRAINER_ID=0 # 当前Trainer节点的编号,范围为[0，PADDLE_TRAINERS_NUM)
"""
```

* 在`.py`文件中使用一个简单的工具函数, `paddle` 就能通过**读取环境变量**来确定当前进程的角色

```python
import paddle.fluid.incubate.fleet.base.role_maker as role_maker
from paddle.fluid.incubate.fleet.parameter_server.distribute_transpiler import fleet

# 根据环境变量确定当前机器/进程在分布式训练中扮演的角色
# 使用 fleet api的 init()方法初始化这个节点
role = role_maker.PaddleCloudRoleMaker()
fleet.init(role) #必不可少的步骤，初始化节点！, 这个初始化放置的位置有啥要求吗?
```



**代码层面**

1. 网络构建的时候和单机的时一致的
2. 单机的 `optimizer` 需要套个 分布式的 `optimizer`. 这是因为分布式和单机的区别就在与参数的更新, 而参数的更新就是在 `optimizer` 部分.

```python
from paddle.fluid.transpiler.distribute_transpiler import DistributeTranspilerConfig

#----------单机代码部分
ctr_model = CTR()
inputs = ctr_model.input_data(params)
avg_cost, auc_var, batch_auc_var = ctr_model.net(inputs,params)
optimizer = fluid.optimizer.Adam(params.learning_rate)
#----------

# 指定分布式的运行模式，通过 DistributeTranspilerConfig进行配置
# 如下，设置分布式运行模式为异步(async)，同时设置参数需要切分，以分配到不同的节点
strategy = DistributeTranspilerConfig()
strategy.sync_mode = False
strategy.runtime_split_send_recv = True
# 配置分布式的optimizer，传入我们指定的strategy，构建program
optimizer = fleet.distributed_optimizer(optimizer,strategy)
optimizer.minimize(avg_cost)
```

3. 上面算是构图完成了, 下面要开始模型训练了.  对于 `pserver` 来说, 只需要开个端口监听着就行了, 而对于 `worker` 来说, 就得乖乖的 `run run run` 了. 

```python
# 根据节点角色，分别运行不同的逻辑
if fleet.is_server():
    # 初始化参数服务器节点, 如果想增量训练, 就 fleet.init_server(model_path)
    fleet.init_server()
    # 运行参数服务器节点
    fleet.run_server()
elif fleet.is_worker():
    # 必不可少的步骤，初始化工作节点！
    # 这边不同的是, 无论时初始化图, 还是执行图, 执行的program都是 fleet. 出来的.
    fleet.init_worker()
    exe = fluid.Executor(fluid.CPUPlace()) # 这个 executor 还是和单机一样哦

    # 初始化含有分布式流程的fleet.startup_program
    exe.run(fleet.startup_program))
    
    # 引入数据读取dataset
    dataset = get_dataset(inputs,params)

    for epoch in range(params.epochs):
        start_time = time.time()
        # 训练节点运行的是经过分布式配置的fleet.mian_program
        exe.train_from_dataset(program=fleet.main_program,
                            dataset=dataset, fetch_list=[auc_var],
                            fetch_info=["Epoch {} auc ".format(epoch)],
                            print_period=10, debug=False)
        end_time = time.time()
        logger.info("epoch %d finished, use time=%d\n" % ((epoch), end_time - start_time))

        # 默认使用0号节点保存模型, 为啥是 0 号节点保存模型 而不是 由 pserver 保存模型?
        if params.test and fleet.is_first_worker():
            model_path = (str(params.model_path) + "/"+"epoch_" + str(epoch))
            fluid.io.save_persistables(executor=exe, dirname=model_path)
    
    # 训练结束，调用stop_worker()通知pserver
    fleet.stop_worker() 
    logger.info("Distribute Train Success!")
    return train_result
```



**inference部分代码**

* 注意, `startup_program` 和 `test_program` 是空的!! 在 `inference` 的时候是建立一个和训练 `program` 完全的独立的一个 `program` 
  * 这个和 `tensorflow` 不一样, 可能是因为 `tf` 可以通过`fetch_list` 执行任意子图. 而在 `paddle` 中, `program` 是最小的执行单位.
* `fluid.unique_name.guard()` : 这个应该是为了重置 `unique_name` 上下文管理器, 来确保新构建的 网络 的名字和 训练网络的名字的 一致性.

```python
startup_program = fluid.framework.Program()
test_program = fluid.framework.Program()
with fluid.framework.program_guard(test_program, startup_program):
    with fluid.unique_name.guard():
        inputs = ctr_model.input_data(params)
        loss, auc_var, batch_auc_var = ctr_model.net(inputs, params)

        exe = fluid.Executor(place)
        feeder = fluid.DataFeeder(feed_list=inputs, place=place)

        fluid.io.load_persistables(
            executor=exe,
            dirname=model_path,
            main_program=fluid.default_main_program())
```

* `auc` 清零, 由于在计算`auc` 的时候`paddle` 创建了一些`Variable`, 用来计算全局的`auc`. 这些状态在 保存模型参数的时候也会被保存下来. 如果想在`inference` 的时候正确计算`auc`, 这些状态需要清零

```python
def set_zero(var_name):
    param = fluid.global_scope().var(var_name).get_tensor()
    param_array = np.zeros(param._get_dims()).astype("int64")
    param.set(param_array, place)

"""
paddle_auc_out, paddle_batch_auc_out, [batch_stat_pos, batch_stat_neg, stat_pos, stat_neg] = fluid.layers.auc(
    prediction, label)
"""

# 这些名字就是对应的4元祖里面的东西.
auc_states_names = [
    '_generated_var_0', '_generated_var_1', '_generated_var_2',
    '_generated_var_3'
]
for name in auc_states_names:
    set_zero(name)
```



