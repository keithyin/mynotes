# Network in Network 论文总结
如有理解错误，请疯狂拍砖！


## Insight
传统CNN的`Convolutional filter`(patch与卷积核内积，之后激活)的结构，对`data patch`抽象能力不行。

- 抽象能力：指的是对同样的概念，卷积层的输出的值应该是不变的。

传统的`CNN`，就简单使用`data patch`与`filter`做内积，然后就求出来的值送入激活函数`ReLU`中。如果表示相同概念的`data patch`稍有不同的话，那么与`filter`做完内积的值也会变化（可能会和之前结果相差甚远），这显然不是我们想要的结果，我们想要不变性。所以作者提出了`NIN`，用来提高卷积层的抽象能力。

## 网络结构

作者对于传统的CNN架构修改了两个部分：

- 作者将原始卷积神经网络中卷积层中的单层感知机替换成了多层感知机（MLP）。结构如下图：
![图](https://pic1.zhimg.com/v2-5417a186eba46617e09768e0c6a22164_b.png)

- 放弃传统`CNN`的全连接层，在最后一层的`feature map`使用`spatial pooling`作为`NIN`的分类的结果。已经证实，全连接层非常容易导致过拟合，极度依赖`dropout`。所以去除全连接层还是有好处的。作者还从另一个角度解释`spatial pooling`：迫使网络的最后一层学习到与类别相关的特征。
![图](https://pic1.zhimg.com/v2-caff8a9b4b115c0156b5d16de97969cc_b.png)

## 理解
作者提出了从两个观点来理解`NIN`：
- 从`feature map` 之间的 `pooling` 的观点来理解：
  - `MLP`第一层的输出就是传统`CNN`在不同`kernel`下 的 `feature map`，`MLP`的第二层就是这些`feature map`进行加权组合的过程。第二层的作用是等价于对这些特征进行`pooling`。

- `feature map`之间参数化的`pooling`层同样等价于一个`1*1`的卷积层。

刚一开始看到这些的时候我是很蒙的，然后我尝试将`NIN`的结构用卷积的形式画一遍：
![](https://pic1.zhimg.com/v2-c456394d01d234ed9ae60c04f2b56050_b.jpg)
发现`NIN`的基本单元是：传统卷积+1\*1卷积核+1\*1卷积核...
看到这之后，`NIN`与1\*1卷积和之间的关系就打通了。
作者还提到了`maxout`，`maxout`的结构图如下：
![](https://pic3.zhimg.com/v2-8d28f173243e72a169d3147c1b72b076_b.jpg)
虽说`maxout`很厉害，可以拟合任意凸函数，但终究敌不过`MLP`（可以拟合任意函数）。

## 思考
为什么要用`NIN`单元代替之前的单层感知机？
假设我们把`data patch`看作一个要进行内容分类的点的话，把单层感知机看作一个`one VS all`的分类器。内容的分布一般不会是线性可分的，所以传统的方法进行分类，产生的`feature map`中会有内容冗余（即：平面画不好，就会有其它内容信息包含进来）所以选了个更好的分类器，这样特征组合的多样化更多。
