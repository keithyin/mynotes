# 学习资料总结

## 博客

* [http://www.wildml.com/](http://www.wildml.com/)
* [https://blog.openai.com/](https://blog.openai.com/)
* [http://colah.github.io/](http://colah.github.io/)
* ​


## CV

[https://www.pyimagesearch.com/](https://www.pyimagesearch.com/)

**公开课**

* udacity : introduction to computer vision
* cs231N



**CNN**

[http://colah.github.io/posts/2014-07-Conv-Nets-Modular/](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)

http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/



[Non-Maximum Suppression (NMS)](https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/)



## NLP

**seq2seq**

* [CTC 与 BeamSearch](https://distill.pub/2017/ctc/)



[RNN LSTM GRU](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)



## EM算法及其应用

> 个人理解： EM 算法将优化目标函数中的 log sum 转成 sum log 更容易优化。

- [EM算法在训练混合高斯模型](http://cs229.stanford.edu/notes/cs229-notes7b.pdf)
- [EM算法](http://cs229.stanford.edu/notes/cs229-notes8.pdf)
- [EM算法用在 PLSA 模型中(Couresa TextMining 3.7)](https://d3c33hcgiwev3.cloudfront.net/_9a5eb7faf6b366c91f6999a05d40b3c9_TM-20-plsa.pdf?Expires=1524873600&Signature=KBwa~vtQ7f8L6XrNPflX7m7VYnLRwIptkH1X1APQVgE991NcnnNHqUBhoXK8f4tf5GtOPTOLbPobdy-7mx1R~BfFOTgtVECkAESCtI3KJMDRITtoWsJUcuxLp4Bkc0icY0tpgzUsr-n~IobNZqDPuCMYV-W-ZzcjGD3GWPQhUeA_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)

## 条件随机场

- 前向后向算法 （统计学习方法，李航）
- 维特比算法（统计学习方法，李航）
- pytorch和 条件随机场相关代码（http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html），文章中提到了一个链接可以看一下。
- [条件随机场](http://www.cs.columbia.edu/~mcollins/crf.pdf)
- ​



## Word2Vec

- [理论https://blog.csdn.net/itplus/article/details/37998797](https://blog.csdn.net/itplus/article/details/37998797)
- [tensorflow 源码解析](https://blog.csdn.net/u012436149/article/details/52848013)





## 机器学习基础

* [深度学习中的各种优化方法](http://ruder.io/optimizing-gradient-descent/index.html)
* [理解偏差与方差的 trade off](http://scott.fortmann-roe.com/docs/BiasVariance.html)
* [Why does Batch Normalization (for deep Neural Networks) fix the vanishing gradient problem?](https://www.quora.com/Why-does-Batch-Normalization-for-deep-Neural-Networks-fix-the-vanishing-gradient-problem)
* [Information Retrivial](https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html)




## 数据结构与算法

**左神公开课两个**

* [https://www.nowcoder.com/live/11](https://www.nowcoder.com/live/11)
* [https://www.nowcoder.com/live/2?page=1](https://www.nowcoder.com/live/2?page=1)



剑指offer

leetcode

