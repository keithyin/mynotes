# 过拟合 及其 解决方法

**什么是过拟合**

* 训练误差非常小，验证集误差非常大

----

**什么原因导致过拟合**

* 模型复杂度比较高，可以完美的拟合训练数据



## 过拟合的解决方法

**从降低模型复杂度的方向考虑**

* 减少参数数量
* L1 正则， L2 正则
* 输入加噪声（等价于 L2 正则）
* 权重加噪声（会使得 激活值非 0 及 1, 这么来控制模型复杂度）

----

**多模型融合的角度**

* dropout
* 集成方法
* 贝叶斯神经网络（对权值采样，然后对预测的结果求平均）

----

**其它**

* 获取更多的数据
* 选择正确的模型

----

**为了 L1 会使得参数稀疏**

考虑整体 `loss_func = some_loss + l1loss` ，两个 **loss** 相加得到一个相对较小的结果。

考虑 **满足数据的 参数的解空间** 时  $\text{l1_norm}(x) = c$   的 $c$ 的最小值。**由于 $\text{l1_norm}(x) = c$ 构成的区域形状比较尖。**  所以导致了稀疏性。



[感觉这个PPT说的更有道理 https://davidrosenberg.github.io/ml2015/docs/2b.L1L2-regularization.pdf](https://davidrosenberg.github.io/ml2015/docs/2b.L1L2-regularization.pdf)

[这个随便看看就好 https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a](https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a)

