# Model Free Control

å›å¿† Planning ä¸­ Policy Iteration ç®—æ³•
* Policy Evaluationï¼šæ ¹æ®å½“å‰policyé‡‡æ ·ä¸€å †æ•°æ®ï¼Œç„¶åè¿›è¡Œ policy evaluation. (Monte-Carlo, TD ?)
* Policy Improvementï¼šæ ¹æ® policy evaluation ç»“æœ è¿›è¡Œ improve policyã€‚Greedy Policy Improvement?
* ğŸ‘†ä¸Šé¢ä¸¤ä¸ªæ“ä½œä¸åœå¾ªç¯

Policy IterationåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šPolicy Evaluation & Policy Improvement, å¯¹äº Policy Evaluationï¼Œæ˜¯æœ‰ modelf-free(MC, TD). å¦‚æœæƒ³è¦æ•´ä½“éƒ½Model Freeçš„è¯ï¼Œé‚£ä¹ˆ Policy Improvement ä¹Ÿéœ€è¦ Model Freeæ–¹æ³•ã€‚

# Policy Improvement

* Greedy Policy improvement over $V(S)$ requires model of MDP
$$
\pi'(s) = \arg \max_{a \in A} \mathcal R_s^a + \mathcal P_{ss'}^aV(s')
$$

* Greedy policy improvement over $Q(s, a)$ is model-free
$$
\pi'(s) = \arg \max_{a \in A} Q(s, a)
$$

å› ä¸º Greedy Policy Improvement over $Q(s, a)$ is model-freeï¼Œæ‰€ä»¥åœ¨ policy evaluationçš„æ—¶å€™ï¼Œæˆ‘ä»¬ evaluate çš„ä¹Ÿæ˜¯ $q(a,s)$ è€Œé $v(s)$ äº†ã€‚



#  on-policy
> * on-policy ä¸ off-policy çš„åŒºåˆ†å‡ºç°åœ¨ policy-evaluation é˜¶æ®µã€‚
> * off-policyä½¿ç”¨ behavior-policy äº§ç”Ÿçš„ trajectoryï¼Œæ¥ evaluate target-policyã€‚

åŸºç¡€çš„ Policy Iterationç®—æ³•ï¼Œåœ¨ policy evaluationæ—¶å€™ï¼Œå› ä¸ºéœ€è¦é‡‡æ ·å¤§é‡episodeï¼Œæ—¨åœ¨æ›´ç²¾ç¡®çš„è¯„ä¼°policyï¼Œæ‰€ä»¥éœ€è¦è€—è´¹å¤§é‡æ—¶é—´ã€‚å¯¹äºcontrolé—®é¢˜æ¥è¯´ï¼Œpolicy evaluationé˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦è€—è´¹é‚£ä¹ˆé•¿æ—¶é—´å—ï¼Ÿ
* ç­”æ¡ˆå½“ç„¶æ˜¯ å¯ä»¥ä¸è´¹é‚£ä¹ˆé•¿æ—¶é—´ï¼Œæ˜¯æœ‰ä¸€ä¸ª episode è¿›è¡Œ policy evaluation å³å¯ï¼Œç„¶åæ‰§è¡Œ policy improvement
* MC On-Policy

Monte-Carloä¸­æˆ‘ä»¬éœ€è¦ä¸€ä¸ª episode è¿›è¡Œ policy evaluationã€‚å¯¹äºTDæ¥è¯´ï¼Œä¸€ä¸ª time-step æˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œ policy evaluation ç„¶å policy improvementäº†ã€‚å¯¹åº”çš„ç®—æ³•ä¹Ÿå«åš Sarsa
* TD On-Policy

# Off-Policy
> * on-policy ä¸ off-policy çš„åŒºåˆ†å‡ºç°åœ¨ policy-evaluation é˜¶æ®µã€‚
> * off-policyä½¿ç”¨ behavior-policy äº§ç”Ÿçš„ trajectoryï¼Œæ¥ evaluate target-policyã€‚

off-policyçš„ä¼˜ç‚¹
* Learn from observing humans or other agents
* Re-use experience generated from old policies $\pi_1, \pi_2, \pi_3, ..., \pi_{t-1}$
* Learn about optimal policy while following exploratory policy
* Learn about multiple policies while following one policy

åœ¨policy-evaluationçš„æ—¶å€™ï¼Œæˆ‘ä»¬çš„ç›®æ ‡ä¸»è¦æ˜¯è®¡ç®— $q_\pi(a,s)$, $q_\pi(a,s)=\mathbb E_\pi[G_t|S_t=s, A_t=a]$. å¦‚æœæˆ‘ä»¬ä½¿ç”¨å…¶å®ƒ policy $\mu$ é‡‡æ ·å‡ºæ¥çš„ trajectory æ¥è¿›è¡Œpolicy evaluationçš„è¯ï¼Œé‚£å°±éœ€è¦ importance sampling
![](https://img-blog.csdnimg.cn/2020123009222658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MzYxNDk=,size_16,color_FFFFFF,t_70)
![](https://img-blog.csdnimg.cn/20201230092022439.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MzYxNDk=,size_10,color_FFFFFF,t_10)
![](https://img-blog.csdnimg.cn/20201230092101210.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MzYxNDk=,size_10,color_FFFFFF,t_10)

