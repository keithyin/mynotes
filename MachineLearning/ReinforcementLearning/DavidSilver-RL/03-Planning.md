Planning : model is known, learn value function / policy from the model

model is known çš„å«ä¹‰æ˜¯ï¼š

* å·²çŸ¥ $\mathcal R_s^a$
* å·²çŸ¥ $\mathcal P_{ss'}^a$

ä¸¤ä¸ªä»»åŠ¡ï¼š

* predictionï¼šç»™å®š policyï¼Œè¾“å‡º value function $v_\pi(s)$
* controlï¼šè¾“å‡º æœ€ä¼˜ policyã€‚

# Policy Evaluation

é—®é¢˜ï¼š ç»™å®š policyï¼Œè®¡ç®—å…¶ value function $v_\pi(s)$
è§£å†³æ–¹æ¡ˆï¼šiterative application of Bellman expectation backup

ç®—æ³•ï¼š
1. at each iteration k+1
2. å¯¹æ‰€æœ‰çš„çŠ¶æ€
3. æ ¹æ® $v_k(s')$ æ›´æ–° $v_{k+1}(s)$, $s'$ æ˜¯ $s$ çš„ä¸‹ä¸€ä¸ªçŠ¶æ€

æ›´æ–°å…¬å¼

$$
v_{k+1}(s) = \sum_{a \in A}\pi(a|s) \Bigr(  \mathcal R_s^a + \gamma\sum_{s' \in S} \mathcal P_{ss'}^{a'}v_k(s') \Bigr)
$$
 
æˆ–è€…æ›´æ–°å…¬å¼
$$
q_{k+1}(s,a)=\mathcal R_s^a+\gamma\sum_{s'\in S}\mathcal P_{ss'}^a\sum_{a'\in A}\pi(a'|s')q_{k}(s',a'))
$$

# Policy Iteration

1. at each iteration j+1
1. è¿›è¡Œ policy evaluationï¼ˆè®¡ç®— $v_{\pi_j}(s)$ï¼‰ï¼ˆå†…éƒ¨éœ€è¦è¿­ä»£ $K$ æ¬¡ æ‰èƒ½å¾—åˆ°æ­£ç¡®çš„ policy evaluation)
2. policy evaluationä¹‹åï¼Œ$v_{\pi_j}(s)$å°±éƒ½çŸ¥é“äº†ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å…¬å¼ improve policyã€‚å¯¹æ‰€æœ‰çš„çŠ¶æ€ ä½¿ç”¨ $v_{\pi_j}(s)$ æ›´æ–° $v_{\pi_{j+1}}(s)$

$$
\pi_{j+1}(a|s) = \max_a \mathcal R_s^a + \gamma\sum_{s'\in S}\mathcal P_{ss'}^a v_{\pi_{j}}(s')
$$
 
æˆ–è€…ä½¿ç”¨ä»¥ä¸‹æ›´æ–°å…¬å¼
$$
\pi_{j+1}(a|s) = \max_a \pi_j(a|s)
$$

ä¿®æ”¹ç‚¹ï¼š
1. policy evaluation æ˜¯å¦çœŸçš„éœ€è¦æ”¶æ•›æ‰å¯ä»¥ï¼Ÿæ‰§è¡Œnæ¬¡ policy evaluationè¿­ä»£æ˜¯å¦å¯è¡Œ


# Value Iteration
> ideaï¼šå¦‚æœæˆ‘ä»¬çŸ¥é“å­é—®é¢˜çš„è§£ $v_{optimal}(s')$ï¼Œé‚£ä¹ˆåŸå§‹é—®é¢˜çš„è§£é€šè¿‡ one-step look ahead ä¹Ÿå¯ä»¥æ‰¾åˆ°.
> value iteration å°±æ˜¯åŸºäºä¸‹é¢è¿™ä¸ªå…¬å¼
> ç›´è§‰ï¼šä»æœ€ç»ˆçš„ reward çš„å¼€å§‹ï¼Œå¾€å‰è¿›è¡Œè¿­ä»£

$$
v_{optimal}(s) = \max_a \mathcal R_s^a + \gamma \sum_{s' \in S} \mathcal P_{ss'}^a v_{optimal}(s')
$$

ç®—æ³•ï¼š
1. at each iteration k+1
2. å¯¹äºæ‰€æœ‰çš„çŠ¶æ€ $s\in S$
3. æ ¹æ®ä¸Šé¢çš„å…¬å¼ï¼Œé€šè¿‡ $v_{k}(s')$ æ›´æ–° $v_{k+1}(s)$

ä¸ policy iteration çš„å¯¹æ¯”ï¼š
1. æ²¡æœ‰æ˜¾å¼çš„ policy
2. ä¸­é—´è¿‡ç¨‹çš„ value functions å¯èƒ½å¹¶ä¸å¯¹åº”ä»»ä½• policy

# Asynchronous Dynamic Programming
å‰é¢èŠçš„å‡ ä¸ªæ–¹æ³•éƒ½æ˜¯ Synchronous backups, å¯ä»¥çœ‹å‡ºï¼šæˆ‘ä»¬éƒ½æ˜¯é€šè¿‡ $v_k(s')$ æ¥æ›´æ–° $v_{k+1}(s)$ çš„ã€‚
é‚£ä¹ˆä»€ä¹ˆæ˜¯ asynchronous backups å¦‚ä½•æ“ä½œå‘¢ï¼Ÿæˆ‘ä»¬ç›´æ¥ inplace çš„æ›´æ–° $v(s)$.

Three simple ideas for asynchronous dynamic programming:
* In-place dynamic programming (inplace æ›´æ–° $v(s)$)
* Prioritised sweeping (å› ä¸ºçŠ¶æ€-å€¼å‡½æ•°çš„æ›´æ–° å’Œ çŠ¶æ€é€‰å–çš„é¡ºåºæ˜¯æœ‰å…³ç³»çš„ï¼Œæ‰€ä»¥å¦‚ä½•é€‰å–çŠ¶æ€æ˜¯ä¸€ä¸ªå€¼å¾—æ€è€ƒğŸ¤”çš„ç‚¹)
* Real-time dynamic programming

## çŠ¶æ€é€‰æ‹©æ–¹æ³•
* Prioritised Sweeping
* ä½¿ç”¨ agent çš„ç»å†å»æŒ‡å¯¼ å½“å‰è¦æ›´æ–°çš„ state çš„é€‰æ‹©

**Prioritised Sweeping**
* ä½¿ç”¨ bellman error æŒ‡å¯¼çŠ¶æ€çš„é€‰æ‹©ï¼Œerror å¤§çš„çŠ¶æ€ä¼˜å…ˆæ›´æ–°ã€‚
$$
\Bigr | \max_a \Bigr (\mathcal R_s^a + \gamma\sum_{s'}\mathcal P_{ss'}^av(s') \Bigr )-v(s) \Bigr |
$$

**Agent's Experience**
* after each time-step $S_t, A_t, R_{t+1}$
$$
v(S_t) \leftarrow \max_a \Bigr( \mathcal R_{S_t}^a + \gamma\sum_{s'}\mathcal P_{S_ts'}^a v(s') \Bigr)
$$

# Full-Width Backups or Sample Backups

## Full-Width Backups
ä¸Šé¢ä»‹ç»çš„æ–¹æ³•å¯ä»¥çœ‹å‡ºï¼š
* DP use full-width backups
* å¯¹äºæ¯æ¬¡ backupï¼ˆçŠ¶æ€çš„æ›´æ–°ï¼‰ï¼Œæ— è®ºæ˜¯syncæ–¹æ³• è¿˜æ˜¯ asyncæ–¹æ³•
    * æ¯ä¸ª successor state and action is considered
    * Using knowledge of the MDP transitions and reward function

DPå¯¹äºç™¾ä¸‡çŠ¶æ€çº§åˆ«çš„é—®é¢˜è¿˜èƒ½è·‘ï¼Œå¦‚æœçŠ¶æ€ç©ºé—´è¿›ä¸€æ­¥æ‰©å¤§ï¼ŒDPå°±å¾ˆéš¾èƒ½è·‘å‡ºç»“æœäº†ã€‚

## Sample Backups
* ä½¿ç”¨ sample rewards and sample transitions $<S,A,R,S'>$ã€‚
* æ›´åŠ ä¾èµ– agent's experience

ä¼˜ç‚¹ï¼š
* Model-Freeï¼šä¸éœ€è¦çŸ¥é“ $\mathcal R_s^a, \mathcal P_{ss'}^a$
* æ¯æ¬¡backupä¸éœ€è¦è€ƒè™‘ æ‰€æœ‰çš„action å’Œ åç»­çŠ¶æ€ï¼Œæ‰€ä»¥æ¯æ¬¡ backupçš„é€Ÿåº¦å¿«
* é€šè¿‡é‡‡æ ·è§£å†³äº† çŠ¶æ€å¤šçš„é—®é¢˜
