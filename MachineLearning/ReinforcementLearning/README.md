# Reinforcement Learning

**ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ **

> Reinforcement Learning is learning what to do -- **how to map situations to actions** -- so as to maximize a numerical reward signal.
>
> **trial-and-error search and delayed reward** -- are the two important distinguishing features of reinforcement learning.
>
> å¼ºåŒ–å­¦ä¹ æ˜¯å®šä¹‰åœ¨ä¸€ç±»é—®é¢˜ä¸Šï¼Œè€Œä¸æ˜¯æ–¹æ³•ä¸Šï¼Œæ‰€æœ‰èƒ½è§£å†³é‚£ç±»é—®é¢˜çš„æ–¹æ³•éƒ½å¯ä»¥ç§°ä½œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚
>
> å¼ºåŒ–å­¦ä¹ å®šä¹‰åœ¨ä»€ä¹ˆçš„é—®é¢˜ä¸Šå‘¢ï¼Ÿ  a learning agent interacting with its environment to achieve a goal.



åœ¨ DRL ä¸­ï¼Œæ·±åº¦å­¦ä¹ é‚£ä¸€å—å¯ä»¥çœ‹ä½œç‰¹å¾æå–å·¥å…·ã€‚

å‡ ä¸ªé‡è¦æ¦‚å¿µï¼š

* stateï¼š The state is sufficient statistic of the environment and thereby comprises all the necessary information for the action to take the best action.


> å¯¹ state çš„ç†è§£ï¼š state æä¾›è¶³å¤Ÿçš„ä¿¡æ¯èƒ½å¤Ÿå¼•å¯¼æˆ‘ä»¬ åšå‡ºæ­£ç¡®çš„action ï¼Œè¿™å°±å¤Ÿäº†ã€‚
>
> å› ä¸º observation ä¸ç­‰ä»·ä¸ stateï¼Œè¿™å°±æ¶‰åŠåˆ°å¦‚ä½•å°† observation ï¼ˆå’Œ actionï¼‰ç¼–ç æˆ state çš„æ–¹æ³•äº†ï¼Œæ„Ÿè§‰åº”è¿™ä¹ˆè€ƒè™‘ï¼š
>
> * å½“å‰è¿™ä¸ªä»»åŠ¡ï¼Œå¦‚æœæƒ³è¦åšå‡ºæ­£ç¡®çš„ actionï¼Œéœ€è¦å“ªäº›ä¿¡æ¯
> * é€šè¿‡å¦‚ä½• å¤„ç† observation å¯ä»¥å¾—åˆ°æ‰€éœ€è¦çš„ä¿¡æ¯ã€‚
>
> ä¸¾ä¸ªä¾‹å­ --> Atari Pongï¼š
>
> * å¦‚æœæƒ³è¦æ­£ç¡®çš„æ§åˆ¶ æŒ¡æ¿ï¼Œæˆ‘ä»¬åº”è¯¥éœ€è¦ å°çƒçš„è¿åŠ¨æ–¹å‘å’Œ è¿åŠ¨é€Ÿåº¦ å’Œ ä½ç½®
> * å•ä¸€å¸§åªèƒ½è·å¾— å°çƒçš„ä½ç½®ï¼Œè¿åŠ¨æ–¹å‘å’Œé€Ÿåº¦éƒ½æ— æ³•è·å–ï¼Œæ‰€ä»¥ç”¨ 4 å¸§æ¥ä»£è¡¨çŠ¶æ€
> * å› ä¸ºä»å››å¸§ä¸­ æ˜¯å¯ä»¥æ¨æ–­å‡ºï¼Œè¿åŠ¨æ–¹å‘ï¼Œä½ç½®ï¼Œé€Ÿåº¦çš„ã€‚


**åºåˆ—å†³ç­–é—®é¢˜çš„ä¸¤ä¸ªåŸºæœ¬ä»»åŠ¡**

* reinforcement learning
    * the environment is initially unknown
    * the agent interacts with the environment
    * the agent improves its policy
* Planning
    * A model of environment is known
    * the agent performs computations with its model (without any external interaction)
    * the agent improves its policy


**Exploration & Exploitation**

* Exploration finds more information about the environment
* Exploitation exploits known information to maximise reward
* It is usually important to explore as well as exploit


## å¼ºåŒ–å­¦ä¹ 

**RLç®—æ³•ä¸‰å¤§ç»„ä»¶**

* Policy: agent's behaviour function
    * it is a map from state to action
    * Deterministic policy: $a=\pi(s)$
    * Stochastic policy: $\pi(a|s) = \mathbb P[A_t=a|S_t=s]$
* Value function: how good is each state and/or action
    * value function is a prediction of future reward
    * Used to evaluate goodness/badness of states
    * And therefore to select between actions, e.g.
$$v_\pi(s) = \mathbb_\pi[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t=s]$$
* Model: agent's representation of the environment
    * model predicts what the environment will do next
    * $\mathcal P$ predicts the next state
    * $\mathcal R$ predicts the next immediate reward
$$
\mathcal P_{ss'}^a = \mathbb P[S_{t+1}=s'|S_t=s, A_t=a]
$$
$$
\mathcal R_s^a = \mathbb E[R_{t+1}|S_t=s, A_t=a]
$$


**RLç®—æ³•åˆ†ç±» ï¼ˆç¬¬ä¸€ç§åˆ†ç±»æ–¹å¼ï¼‰**

* value basedï¼š--- ç°åœ¨åŸºæœ¬å°±æ˜¯ Q-Learning äº†
    * æ˜¾å¼ policy å‡½æ•° ï¼ˆæ— ï¼‰
    * Value Function ï¼ˆæœ‰ï¼‰
* policy based --- policy gradient å’Œ actor-critic
    * æ˜¾å¼ policy å‡½æ•° ï¼ˆğŸˆ¶ï¸ï¼‰
    * æ˜¾å¼ value function ï¼ˆğŸˆšï¸ï¼‰

* Actor Critic
    * æ˜¾å¼ policy function ï¼ˆğŸˆ¶ï¸ï¼‰
    * æ˜¾å¼ value function ï¼ˆğŸˆ¶ï¸ï¼‰

**RLç®—æ³•åˆ†ç±» ï¼ˆç¬¬äºŒç§åˆ†ç±»æ–¹å¼ï¼‰**

* model freeï¼š
    * policy and/or value function ï¼ˆğŸˆ¶ï¸ï¼‰
    * model ï¼ˆğŸˆšï¸ï¼‰
* model basedï¼š
    * policy and/or value function ï¼ˆğŸˆ¶ï¸ï¼‰
    * model ï¼ˆğŸˆ¶ï¸ï¼‰

**RLç®—æ³•åˆ†ç±» ï¼ˆç¬¬äºŒç§åˆ†ç±»æ–¹å¼ï¼‰**

* on-policy

* off-policy

**ç®—æ³•çš„åŸºæœ¬ç»„ä»¶**

* MC
* TD(0)
* TD($\lambda$)



**ç®—æ³•çš„ä¸¤ç§æ€§è´¨**

* on-policy
* off-policy




**ä¸¤ç±»é—®é¢˜**

* prediction
* control



**ç®—æ³•æ´¾ç³»**

* Q-Learning
  * Q-Learning
  * Deep Q-Learning
  * Double Deep Q-Learning
  * Prioritized Experience Replay
* Policy-Based
  * Actor-Critic
  * REINFORCE
  * A3C
  * DPG
  * DDPG
  * 


**å¦ä¸€ç§åˆ†ç±»æ–¹æ³•**

* DQN
  * MC
  * TD
  * on-policy / off-policy
  * stochastic / deterministic policy
  * discrete / continuous policy
* Policy Gradient
  * MC
  * TD
  * on-policy / off-policy
  * stochastic / deterministic policy
  * discrete / continuous policy





## DRL é¢ä¸´çš„é—®é¢˜

* ç›‘ç£ä¿¡å·åªæœ‰ä¸€ä¸ª rewardï¼Œè€Œä¸”ååˆ†ç¨€ç–
* agent çš„ observation æ˜¯æ—¶åºç›¸å…³çš„ï¼Œå¹¶ä¸æ˜¯ iid çš„ã€‚
  * è¿™ä¸ªé—®é¢˜æ˜¯è¿™æ ·ï¼šä¼ ç»Ÿçš„ RL ç®—æ³•ï¼Œéƒ½æ˜¯çœ‹åˆ°ä¸€ä¸ª obsï¼Œç„¶åç›´æ¥å°±æ›´æ–°å‚æ•°ï¼Œä½†æ˜¯ DL éœ€è¦è®­ç»ƒæ•°æ®æ˜¯ IID çš„ã€‚ç”¨ä¼ ç»Ÿ RL çš„è®­ç»ƒæ–¹æ³•æ˜¾ç„¶æ˜¯ä¸è¡Œçš„å•¦ï¼Œæ‰€ä»¥æå‡ºäº† experience replay æ–¹æ³•ã€‚
  * ä¸ºä»€ä¹ˆ DL éœ€è¦çš„è®­ç»ƒæ•°æ®æ˜¯ IID çš„å‘¢ï¼Ÿ å¯èƒ½çš„åŸå› æ˜¯ï¼šå› ä¸ºæˆ‘ä»¬ç”¨ mini-batch è®­ç»ƒæ–¹æ³•ï¼Œä¸€ä¸ª mini-batch çš„æ¢¯åº¦åº”è¯¥æ˜¯ æ•´ä¸ª batch çš„æ— åä¼°è®¡ï¼Œæ•°æ® IID çš„è¯ï¼Œæ˜¯ æ— åï¼Œä½†æ˜¯å¦‚æœæ•°æ®ä¸æ˜¯ IID çš„è¯ï¼Œé‚£å°±ä¸æ˜¯ æ— åäº†ã€‚
* å¦‚æœä¸å¥½å®šä¹‰ rewardï¼Œå°±åŸºæœ¬ä¸Šæ­‡èœäº†




## Policy

Policy æœ‰ä¸¤ç§å±æ€§ï¼š

* continuousï¼Œ discrete
* stochasticï¼Œdeterministicï¼ˆå¯¹äº deterministic policy çš„ç®—æ³•ä¸€èˆ¬è¦æ±‚ policy æ˜¯ continuousï¼‰





## Value Function

Value Function æœ‰ä¸¤ç§ï¼š **ç›®çš„æ˜¯æ±‚ bellman equation**

* state-value Function
* action-value Function




## on/off policy

* off-policyï¼šå¯ä»¥ä¿è¯ exploration




## on/off-line

* â€‹




## MC-TD bias Varianceï¼š

* ä¸ºä»€ä¹ˆè¯´ MC æ–¹æ³• 0-biasï¼Œ high variance
* ä¸ºä»€ä¹ˆè¯´ TD(0) æ–¹æ³• low-biasï¼Œlow variance



**ä»å€¼ä¼°è®¡çš„è§’åº¦æ¥ç†è§£ï¼Ÿ**

å‡è®¾ trajectory $\tau = \{s_0,a_0,s_1,a_1,...\}$ 
$$
p(\tau) = p(s_0)\prod_{t=0}^T \pi(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

$$
G(s_t)=\sum_t^T r_{t+1}+\gamma r_{t+2} + ... + \gamma^{T-t-1} r_T
$$

$$
R(s_t) = \mathbb E\Bigr[G(s_t)\Bigr]
$$

æœ€åä¸€ä¸ªå¼å­æ˜¯å¯¹ trajectory çš„æœŸæœ›ã€‚

MC æ–¹æ³•æ˜¯é‡‡ä¸€ä¸ª trajectoryï¼Œæ‰€ä»¥æ˜¯å¯¹ Value çš„æ— åä¼°è®¡ã€‚ä½†æ˜¯ä¸ºä»€ä¹ˆæ–¹å·®å¤§å‘¢ï¼Ÿå› ä¸º trajectory ä¼šè·‘åï¼Ÿ

ä¸ºä»€ä¹ˆ TD(0) æ–¹å·®å°ï¼Ÿ TD target  $r_{t+1}+V(s')$  ï¼Œ$s'$ çš„å–å€¼ä¹Ÿæ˜¯æœ‰ä¸€ä¸ªåˆ†å¸ƒçš„å§ï¼Œä¸è¿‡è¿™ä¸ªä¼¼ä¹æ¯” trajectory æ–¹å·®è¦å°ä¸€ç‚¹ï¼Œä½†æ˜¯å¼•å…¥äº†åå·®ï¼Œå› ä¸º $r_{t+1} + V(s')$ å¹¶ä¸æ˜¯ $V(s)$ çš„æ— åä¼°è®¡ï¼Œåªæœ‰ $V(s)=r_{t+1}+V(s')$ æ—¶ï¼Œæ‰æ˜¯æ— åä¼°è®¡ã€‚ 



ä» trajectory çš„è§’åº¦æ¥ç†è§£ï¼Ÿ



## å‡å° variance çš„æ–¹æ³•

> å‡å° variance ä¸€èˆ¬æ˜¯å¯¹äº MC æ–¹æ³•è€Œè¨€çš„ï¼Œå› ä¸º MC æ–¹æ³•æ–¹å·®å¤§ã€‚

* ç”¨ TD(0), ä¸ç”¨ MC æ–¹æ³•
* reward ä¹˜ä¸ªç³»æ•°



**reward ä¹˜ä¸ªå°äº 1 çš„ç³»æ•°**

è¿™ä¸ªæ–¹æ³•çš„ç›´è§‚è§£é‡Šæ˜¯ï¼ŒMC é‡‡æ ·ï¼Œè¶Šå¾€åæ–¹å·®è¶Šå¤§ï¼ˆä¸€æ­¥é”™ï¼Œæ­¥æ­¥é”™ï¼Œå°±æ˜¯è¿™ç§æ„Ÿè§‰ï¼‰ï¼Œç„¶åå¯¹ discounted reward å†è¿›è¡Œ discountingã€‚ ä½†æ˜¯è¿™ä¸ªæ–¹æ³•å¼•å…¥äº† åå·®ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ

state-value çš„å®šä¹‰æ˜¯ï¼š
$$
V(s_t) = \mathbb E\Biggr[r_{t+1}+\gamma r_{t+2}+...\Bigr|s_t\Biggr]
$$
ç›´æ¥ç”¨é‡‡æ ·çš„ç»“æœæ¥è®¡ç®— $V(s_t)$ çš„è¯ï¼Œæ˜¯æ— åä¼°è®¡ï¼Œä½†æ˜¯ å¯¹äºreward å†è¿›è¡Œä¸€æ¬¡ discount çš„è¯ï¼Œä¼°è®¡çš„å°±ä¸æ˜¯æ— åä¼°è®¡äº† ï¼Œæ‰€ä»¥ä¼šæœ‰åå·®ã€‚



## Learning & Planning & Search

* Learning : **model is unknown**, learn value function / policy from the experience
* Planning : **model is known**, learn value function / policy from the model
* Search : select the best action of current state by **lookahead**

**Search:** å¦ä¸€ç§ Planning çš„æ–¹æ³•ã€‚

* ä¸ç”¨æ±‚è§£æ•´ä¸ª MDPï¼Œ åªéœ€æ±‚è§£ sub-MDPï¼ˆfrom now onï¼‰
* â€‹




## Glossary

* prediction problem : ä¹Ÿå«åš policy evaluationã€‚ç»™å®šä¸€ä¸ª policyï¼Œ è®¡ç®— state-value function æˆ– action-value function çš„å€¼ã€‚
* control problem ï¼š å¯»æ‰¾æœ€ä¼˜ policy
* Planningï¼šæ ¹æ® model æ„å»ºä¸€ä¸ª value function æˆ–è€… policyã€‚ï¼ˆmodelå·²çŸ¥å“¦ï¼‰
* on-policyï¼š evaluate or improve the behavior policy
* off-policy ï¼šä» behavior policy å½¢æˆçš„ traces ä¸­å­¦ä¹  è‡ªå·±çš„æœ€ä¼˜ policy
* online mode/updateï¼štraining algorithms are executed on data acquired in sequenceã€‚
* offline mode/updateï¼šä¹Ÿå« batch modeï¼Œå¹¶ä¸æ˜¯çœ‹åˆ°ä¸€ä¸ªæ ·æœ¬å°±è®­ç»ƒã€‚å®Œæˆä¸€ä¸ª epoch å†æ›´æ–°ä¹Ÿå« off-line
* model-freeï¼š agent ç›´æ¥ä» experience ä¸­å­¦ä¹ ï¼ŒmodelæœªçŸ¥ï¼ˆä¸çŸ¥é“ çŠ¶æ€è½¬ç§»çŸ©é˜µï¼‰


* episodicï¼šç¯å¢ƒæœ‰ä¸ªç»ˆæ­¢çŠ¶æ€ï¼ˆè€Œä¸”ä¸€å®šä¼šåˆ°è¾¾è¿™ä¸ªç»ˆæ­¢çŠ¶æ€ï¼‰
* non-episodicï¼š ç¯å¢ƒæ²¡æœ‰ç»ˆæ­¢çŠ¶æ€ï¼ˆæˆ–è€…ä¸ä¼šåˆ°è¾¾ä¸€ä¸ªç»ˆæ­¢çŠ¶æ€ï¼‰ï¼ˆMCçš„æ–¹æ³•æ­‡èœï¼‰
* reparameterization trick : [https://www.quora.com/What-is-the-reparameterization-trick-in-variational-autoencoders](https://www.quora.com/What-is-the-reparameterization-trick-in-variational-autoencoders)
* [path-wise derivative](http://www.mathfinance.cn/pathwise-derivative-vs-finite-difference-for-greeks-computation/)
* â€‹stationary environment : 
* changing environment :
* episode: ä»å¼€å§‹åˆ°ç»“æŸä¸€ä¸ªçš„ trajectory
* trajectoryï¼šè½¨è¿¹ï¼Œä»»æ„è¿ç»­æ—¶åˆ»éƒ½å¯ä»¥æ„æˆ trajectory
* revealed information :  æ­éœ²çš„ä¿¡æ¯
* side information ï¼š(additional variables that are not predicted, but are related to variables that are predicted)
* â€‹




## æ¨å…¬å¼çš„ å‡  ä¸ªtrick

* å¯¹æœŸæœ›æ±‚å¯¼æ—¶ï¼Œå°†æœŸæœ›æ”¹æˆ æ±‚å’Œå½¢å¼
* score function
* å°†å¯¹ trajectory çš„æ“ä½œå˜æˆå¯¹ state-action çš„æ“ä½œã€‚ 






## å‚è€ƒèµ„æ–™

[David Silver](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)

[Berkely RL CS294](http://rll.berkeley.edu/deeprlcourse/#lecture-videos)

[Pong From Pixels karpathy](http://karpathy.github.io/2016/05/31/rl/)

[https://www.nervanasys.com/deep-reinforcement-learning-with-neon/](https://www.nervanasys.com/deep-reinforcement-learning-with-neon/)

