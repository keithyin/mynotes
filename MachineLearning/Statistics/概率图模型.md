# chapter02

## 有向无环图

如何根据一个图对联合分布进行分解

* 根据图的**拓扑序**给每个顶点进行标号，然后按照以下方式进行分解就可以了。

 $p(x_1, x_2, ..., x_3) = \prod_{i=1}^n p(x_i| x_{\pi_i})$

* 其中：$x_{\pi_i}$ 表示节点 $x_i$ 的父节点



当判断 两个节点[集] 是否 [条件]独立的时候，就看两个节点[集] 之间是否有通路。**这个通路不考虑有向图边的方向**， 是否有通路是根据图的子结构的决定的，（线性结构，V-结构，$\Lambda$-结构）。不同的结构具有不同的判断路是不是通的方法。



## 无向图

使用最大割来定义联合概率的分解

$p(x) = \frac{1}{Z} \prod_{c \in \mathcal C} {\psi_{X_C}(x_C)}$



# chapter03



# chaper05: 统计学概念

* 概率论：给定概率模型（`a graph and a set of local conditional probabilities or potentials`），如何根据概率模型进行概率计算。 `model -> data`
* 统计学：概率模型是未知的(`graph 已知，但是条件概率或者pontentials 未知。有时甚至 graph 都未知`)，需要根据观测的数据得到概率模型 （统计学从某种方面来说可以看做概率论的反操作）。 `data -> model`
  * 频率学派
  * 贝叶斯学派

统计学要解决的核心问题：1）密度估计，2）回归，3）分类



### 贝叶斯学派

> attempt to deny any fundamental distinction between probability theroy and statistics
>
> probability theory itself provides the capability for inverting relationships between uncertain quantities (**bayes rule**)
>
> bayesian statistics represents an attempt to treat all **statistical inference** as **probabilistic inference**.

**贝叶斯参数估计**
$$
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}
$$

* 可以看出，贝叶斯参数估计实际是产出了参数 $\theta$ 的一个概率分布，而不是一个值
* 如果我们想要获得一个**值**的话，需要一些额外的准则（`mean, mode`）

* 

### 频率学派

> The frequentist approch wishes to avoid the use of prior probabilities in statistics, and thus avoids the use of bayes rule for the purpose of assigning probabilities to parameters.
>
> 频率学派的目标是建立一个客观的统计理论，这样，不同的统计学家在同一个数据集上得到的结论是相同的。
>
> 贝叶斯学派就偏主观，因为对于先验分布的选择是主观的，这样不同的统计学家在同一个数据集上可能得到不同的结论。

已抛硬币的正面概率 $\theta$ 来讲：

* 频率学派：$\theta$ 是硬币的一个固有属性，给这个值一个概率是不 make sense 的。
* 贝叶斯学派：认同 $\theta$ 是硬币的一个固有属性，但同时也认为 $p(\theta)$是用来表示统计学家的不确定性，而不是用来表示硬币的物理性质。

### 一堆 estimator

关于 贝叶斯学派 和 频率学派如何看待 $p(x|\theta)$

* 一个直观的解释为： $p(x|\theta)$ ，assigning probability to $X$ for each fixed value of $\theta$
* 贝叶斯学派：$p(x|\theta)$ , 一个条件概率分布。（这种理解实际上是将 $\theta$ 看成了随机变量）
* 频率学派： $p(x|\theta)$, 将其看做一族与$\theta$相关的概率分布，（with no implication that we are conditioning on $\theta$）

当我们将 $p(x|\theta)$ 看做 固定了 $x$ 的，一个关于 $\theta$的函数，那么 $p(x|\theta)$就被称为 `likelihood function` 

给定一个 $\theta$ 值，我们可以使用似然函数进行评估。

**最大似然估计 (maximum likelihood estimate)**

> 遍历所有可能的 $\theta$ ，选择导致似然函数取得最大值的 $\theta$, 就构成了最大似然估计 estimator

$$
\hat \theta_{ML} = \arg\max_\theta p(x|\theta)
$$



**bayes estimate**
$$
\hat \theta_{Bayes} = \int \theta p(\theta|x)d\theta
$$
**maximum a posteriori estimate**
$$
\begin{aligned}
\hat \theta_{MAP} &= \arg\max_{\theta} p(\theta|x)\\\\
&=\arg\max_{\theta}p(x|\theta)p(\theta)
\end{aligned}
$$

* 从该公式可以看出，如果先验概率取 $p(\theta)$ 为 `uniform distribution` 的话，最大后验估计退化为最大似然估计
* 当先验概率$p(\theta)$ 不取 `uniforma distribution`的话，可以将最大后验估计看做为 `penalized likelihood`. 带惩罚项的最大似然估计常被频率派统计学家用在样本量很少的场景中。

$$
\hat \theta_{MAP} = \arg\max_{\theta} \{\log p(x|\theta) + \log p(\theta)\}
$$

> 从 MAP 上似乎难以看出来贝叶斯估计与频率估计区别，因为 $p(\theta)$ 既可以看做先验，又可以看做 panalty



### prediction

从 `prediction` 任务上来看贝叶斯学派与频率学派的区别

**贝叶斯学派**
$$
\begin{aligned}
p(x_{new}|x) &= \int p(x_{new}, \theta|x) d\theta\\\\
&=\int p(x_{new}|\theta, x)p(\theta|x)d\theta\\\\
&=\int p(x_{new}|\theta)p(\theta|x) d\theta
\end{aligned}
$$

* 由于将 $\theta$ 看做一个随机变量，所以这是一个积分。贝叶斯学派认为，相比于只用一个点进行估计，积分提供了一个更好的估计。

**频率学派**

$p(x_{new}|\hat\theta_{MAP}), p(x_{new}|\hat\theta_{ML})$

* 不认为 $\theta$ 是随机变量



### 密度估计

> 我们有随机变量 $X$ 的一堆观测值，我们希望通过这些观测值归纳出 $X$ 的概率密度。这就是密度估计
>
> 给定一个概率密度模型，然后用观测数据估计其参数值。
>
> 三类模型：
>
> * 参数模型，parametric models
> * 混合模型，mixture models
> * 非参模型，nonparametric models



得到了 $X$ 的概率模型之后，我们可以通过该模型来判断一个 $X$ 的观测值是否 `typical` . 这个判断可以用在多个任务上，比如：`fault detection, outlier detection and clustering` 。密度估计也可以用在 *dimensionality reductions* 问题上。回归和分类实际也是在进行密度估计（条件概率密度估计）

**一元高斯密度估计**

> 这里主要关注的是如何使用概率图模型来表示统计问题。实际就是，关于随机变量 $X$ 的 N 个观测数据实际就是概率图中的 $N$ 个 `node`. 一般用 $\{X_1, X_2, .., X_N\}$ 表示。

* 认为模型的参数是一个值，我们使用最大似然估计来估计该值



**贝叶斯一元高斯密度估计**

> 使用概率图模型表示贝叶斯高斯密度估计的话，相比上边的概率图模型，这里多了要给节点，就是随机变量节点 $\theta$ 

* 认为模型参数是一个分布，使用贝叶斯估计的方式得到参数的后验分布



 **离散数据的密度估计**

> 有限取值的离散随机变量的密度估计



**离散数据贝叶斯密度估计**

> 迪利克雷分布作为参数的先验

关于 β分布 与 迪利克雷分布 先验如何确定其超参数的值，就需要思考，到底需要多少的样本，才使得样本的统计特性 dominate 概率分布！！！



**mixture models**

* 如果一个随机变量的值范围 $[0, \inf)$, 那么使用高斯作为其密度模型显然是不自然的，因为高斯分布的支持集是整个实数域，这时候我们可能考虑的概率分布就是 *gamma* 分布，或者 *lognormal* 分布，这俩的 support 就是 $[0, \inf)$
* 同样，*multinomial* 分布将离散值看作无序的，有限的值集合。对于有序的，无限的离散数据来说，那么 *poisson/geometric* 或许是一个更好的选择
* 上述的分布都有其对应的 最大似然/贝叶斯估计，他们从属于一个更大的分布家族-*exponential-family* . 



如果是多峰的分布，无法使用单一的 *gaussian，gamma，lognormal* 来表示出来的话，那该怎么办呢？**mixture models**





**nonparametric density estimation（非参密度估计）**





### 回归

> 目标是建模 *response/output* 变量与 *covariate/input* 变量之间的关系，我们可以通过条件概率分布来捕捉这种关系 $p(y|x) $ ，但是从回归的角度来讲，总不能最终给出一个概率分布吧。所以一般使用 $E[y|x]$ 作为最终的结果


$$
\begin{aligned}
 Y_n = \beta^Tx_n + \epsilon_n \\\\
 E[Y_n|X_n] = \theta^Tx_n
\end{aligned}
$$

* 线性回归模型将 $Y_n$ 建模为两个部分的和： 1）完全确定性的基于输入的计算，2）完全随机的与输入无关的随机变量
* 线性回归通常说的就是将 $\epsilon_n$ 看作随机变量，其分布为 $\mathcal N(0, \sigma^2)$, 方差用来表征条件均值的 `variation`



**conditional mixture models**
$$
p(y_n|x_n, \theta) = \sum_{k=1}^K p(z_n^k=1|x_n,\theta)p(y_n|z_n^k=1, x_n, \theta)
$$
每个 mixture component $p(y_n|z_N^k=1, x_n)$ 对应一个线性回归模型，每个 mixture proportions $p(z_n^k=1|x_n)$ 跟据 $x_n$ 的值来在不同的线性回归模型中进行切换，也就是说 mixture proportions 根据不同的输入区间选择不同的线性回归模型



**bayesian approaches to regression**

可以给模型的参数或者整个*conditional mean function* 一个先验概率分布，然后使用贝叶斯的方法进行估计



### 分类

> 分类和回归问题非常类似，都包含一个pair。两者不同的地方是，分类问题的 response variable 是一个有限集合，而回归问题的 response variable是一个无限集合。这看起来不起眼的不同有着非常重要的隐喻

分类问题两种基本解法

* generative model
* discriminative model



符号表示

* $X$: feature vector
* $Q$: label



**generative model**

* 从概率图的角度讲，`there is an arrow from Q to X`. 这种方法与密度估计非常类似。对不同的变量 $Q$, 我们有个密度 $p(x|q)$, 我们称之为 *class-conditional density* , 我们同样也需要一个 *marginal probability* $p(q)$, 称之为*类别Q的先验概率*(在观测到X之前，类别的概率)。如果我们想要 *invert the arrow*（$p(q|x)$），那么先验概率是必然需要的。

**discriminative model**

* 这个方法和回归非常像，直接建模 $p(q|x)$ 



# chapter06：线性回归

> Linear regression model expresses $Y_n$ in terms of input-independent random variation $\varepsilon_n$ around the conditional mean $\beta^Tx_n$ . 对于 $\varepsilon_n$ 分布的选择等价与概率密度估计时对于概率密度模型的选择，线性回归通常指的是 $\varepsilon_n \sim N(0, \sigma^2)$

$$
y_n = \theta^Tx_n + \varepsilon_n
$$

* $\theta$ 为参数向量
* $\varepsilon_n$ 作为*error term*：we don't necessarily expect to be able to perfectly as a linear function



当我们给 *error term* 一个概率假设的时候，我们就能得到线性回归的一个概率解释了。

* 假设：$\varepsilon_n \sim N(0, \sigma^2)$ , 那么 $y_n \sim N(\theta^Tx_n, \sigma^2)$
* 然后使用几大似然估计，我们依旧可以得到 *mse* 损失函数

> 使用高斯分布作为 *error term* 的合理性可以从 *central limit theorem* 的角度来思考，$\varepsilon_n$ can be decomposable into sums of many small random terms。



> NOTE: 频率派的估计方法并不局限于 似然函数，其实 *least square cost function* 也算是一个 *estimator*
>
> We can define *least square estimator* of a parameter as a value that minimizes the least-squares cost function, **whether or not** the underlining probability model involves a gaussian assumption.
>
> mse 和 极大似然可以看做 两个竞争者。只是当 underlying probability 是高斯分布的时候两者形式一样而已。



**概率视角可以指导我们设计更复杂的模型！**



# chapter07：线性分类

> 概览：
>
> 1. 说明了为啥不能直接用回归解决分类问题
> 2. 从生成模型角度说明了 为什么 *sigmoid、softmax*
> 3. 因为生成模型的 *class-conditional density* 不好指定，所以直接判别式借用 *sigmoid、softmax*函数形式直接开搞
>
> 疑问：为啥老是会提到条件期望呢？



**可以用线性回归解决分类问题吗？**





sigmoid 函数是怎么得来的呢？



**gaussian class-conditional densities**

考虑分类问题的生成模型：

* 类别的 *marginal distribution*： $p(y|\pi) = \pi^y(1-\pi)^{1-y}$
* 特征值的条件概率：每个特征值的条件概率分布都是高斯分布。（这些高斯分布，方差一样，均值不同）

$$
\begin{aligned}
p(x_j|Y=0, \theta_j) &= \frac{1}{(2\pi\sigma^2)^{0.5}}\exp\Bigr\{-\frac{(x_j-\mu_{0j})^2}{2\sigma_j^2}\Bigr\} \\\\
p(x_j|Y=1, \theta_j) &= \frac{1}{(2\pi\sigma^2)^{0.5}}\exp\Bigr\{-\frac{(x_j-\mu_{1j})^2}{2\sigma_j^2}\Bigr\}
\end{aligned}
$$

* 联合分布为

$$
p(x, y| \theta) = p(y|\pi)\prod_{j=1}^m p(x_j|y,\theta_j)
$$

* 下面考虑计算后验概率分布 $p(Y=1| x, \theta)$, 方便起见，我们使用一些矩阵符号

$$
p(x|y=k, \theta)=\frac{1}{(2\pi)^{0.5}|\Sigma|^{0.5}} \exp\Bigr\{-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)\Bigr\}
$$



$$
\begin{aligned}
p(y=1|x,\theta) &=\frac{p(x|Y=1, \theta)p(Y=1|\pi)}{p(x|Y=1, \theta)p(Y=1|\pi) + p(x|Y=0, \theta)p(Y=0|\pi)} \\\\
&=\frac{\pi \exp\{-0.5(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\}}{\pi \exp\{-0.5(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\} + (1-\pi) \exp\{-0.5(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\}} \\\\
&=\frac{1}{1+\exp\{-\log\frac{\pi}{1-\pi} + 0.5(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) - 0.5(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\}}\\\\
&=\frac{1}{1+\exp\{-(\mu_1-\mu_0)^T\Sigma^{-1}x + 0.5(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0) - \log\frac{\pi}{1-\pi}\}} \\\\
&=\frac{1}{1+\exp\{-\beta^Tx - \gamma\}}
\end{aligned}
$$
其中：
$$
\begin{aligned}
\beta &= (\mu_1-\mu_0)^T\Sigma^{-1} \\\\
\gamma &= -0.5(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0) + \log\frac{\pi}{1-\pi}
\end{aligned}
$$

可以看出，$Y=1$ 的后验概率形式就是 *sigmoid* 函数。类似的，*softmax* 也可以通过相同的方式得到



# chapter08: 指数族 & generalized linear models

> 指数族与广义线性模型之间的关系：
>
> * 指数族是一族概率分布
> * 广义线性模型是用来参数化这些概率分布的
>
> 线性回归与线性分类属于广义线性模型的特例。



## 指数族

如果一个概率密度函数可以写成以下形式，那么该概率分布就属于指数族
$$
\begin{aligned}
p(x|\eta) &= h(x)e^{\eta^TT(x)-A(\eta)} \\\\
&= \frac{h(x)e^{\eta^TT(x)}}{e^{A(\eta)}} \\\\
&= \frac{1}{Z(\eta)}h(x)e^{\eta^T(x)}
\end{aligned}
$$

* $\eta$: 参数向量，称之为 *natural parameter*
* $T(X)$: *sufficient statistic*
* $h(x)$: 这玩意并不重要
* $A(\eta)=\log \int h(x)e^{\eta^TT(x)}dx$
* $A(\eta) = \log Z(\eta)$



**Bernoulli distribution**
$$
\begin{aligned}
p(x|\pi) &= \pi^x(1-\pi)^{1-x} \\\\
&= e^{\log(\frac{\pi}{1-\pi})x + \log(1-\pi)}
\end{aligned}
$$

* $\eta=\log\frac{\pi}{1-\pi}$， $\eta$ 和原始分布中的参数 $\pi$ 是可互相转换的 $\pi=\frac{1}{1+e^{-\eta}}$
* $T(x)=x$
* $A(\eta) = -\log(1-\pi) = \log(1+e^\eta)$
* $h(x)=1$



**Poisson distribution**
$$
\begin{aligned}
p(x|\lambda) &= \frac{\lambda^xe^{-\lambda}}{x!} \\\\
&= \frac{1}{x!}e^{\log(\lambda)x - \lambda}
\end{aligned}
$$

* $\eta=\log(\lambda)$, $\eta$ 与原始概率分布参数也是可以相互转换的 $\lambda = e^\eta$
* $T(x)=x$
* $A(\eta) = \lambda = e^\eta$
* $h(x)=\frac{1}{x!}$



**Gaussian distribution**
$$
\begin{aligned}
p(x|\mu, \sigma^2) &= \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\\\\
&=\frac{1}{\sqrt{2\pi}}e^{\frac{\mu}{\sigma^2}x - \frac{1}{2\sigma^2}x^2 - \frac{\mu^2}{2\sigma^2} - \ln\sigma}
\end{aligned}
$$

* $\eta=[\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}]^T$
* $T(x)=[x, -x^2]^T$
* $A(\eta) = \frac{1}{2\sigma^2} + \ln\sigma = -\frac{\eta_1^2}{4\eta_2} - \frac{1}{2}\ln(-2\eta_2)$



**multinomial distribution**

> 有一个 trick 需要注意哦



指数族分布有一个特点就是：通过对 $A(\eta)$ 求导，我们可以得到分布的 *moments* .一阶导是均值，二阶导是方差

* 一阶导：$E[T(x)]$
* 二阶导：$Var[T(x)]$



> 由于 *canonical parameter* $\eta$ 和原始分布的均值之间是一一映射的关系，所以我们可以写成 $\eta = \psi(\mu)$

* 上述映射说明了，a distribution in the exponential family can be parameterized not only by $\eta$ but also $\mu$
* $\eta$: *canonical parameterization*
* $\mu$: *moment parameterization*

> 实际上，我们之前定义概率分布的时候，大都是使用 moment 作为其参数的，比如 bernolli distribution，Gaussian distribution, 等等。我们上面将伯努利分布写成指数族形式，实际上就是将 moment 参数映射成 canonical 参数。





## 广义线性模型

> Generalized linear models

概率图结构: $X_n \rightarrow Y_n$

回忆在线性回归/判别式线性分类中我们是怎么搞的。

1. 假设 观测数据 $x$ 通过一个线性组合后进入模型。$\xi = \theta^Tx$
2. 选择关于 **$Y$ 的条件期望**的一个表示。$\mu = f(\xi)$，对于线性回归，$f()$是 *identity function*, 对于二分类问题，$f()$ 是 *sigmoid/cumulativeGaussian* 。
3. 然后再给 $Y$ 一个条件概率。对于线性回归，是 高斯分布。对于二分类，是伯努利



> 对于广义线性模型来说，$Y$ 的条件概率就不会局限于 高斯/伯努利/类别 分布了。$Y$的条件概率可以扩展到指数族中。那就有一堆分布可以用了。



相比于之前的2步，在广义线性模型我们需要3步

1. 假设 观测数据 $x$ 通过一个线性组合后进入模型。$\xi = \theta^Tx$
2. 条件期望 $\mu$ 可以被表示为 $f(\xi)$ ，该函数仅是对 $\xi$ 的线性组合。$f()$ 被称作为 *response function*
3. 给 $Y$ 指定一个均值为 $\mu$ 的条件概率，该概率分布需要属于指数族 （此时还需要确认 $\eta=\psi(\mu)$）. $\eta=\psi(f(\theta^Tx))$

> 除了最后一步，条件概率分布可以在整个指数族里面选，其余也没啥变化



在 *GLM* 框架下，it is convenient to work with a slight variation on the exponential family theme. In particular, in the GLM framework we assume the conditional distribution of *x* takes the following form
$$
p(x|\eta, \phi) = h(y, \phi) e^{\frac{\eta^Tx - A(\eta)}{\phi}}
$$
对比着高斯分布来看 
$$
\begin{aligned}
p(x| \mu, \sigma^2) &= \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2} \\\\
&= \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-x^2 + 2\mu x}{2\sigma^2}} * e^{-\frac{\mu^2}{2\sigma^2}}
\end{aligned}
$$

> 可以看出，我们是可以将 $\mu, \sigma$ 给搞到一个参数向量里面去的。但是分开两种参数的方式看起来更加 *natural* . 一般而言，即使 *two-parameter exponential familly* 表示不了 $p(x|\eta, \phi)$ 也无所谓。这就给了我们极大的灵活性。



在建模一个 *GLM* 时，有两个点是需要我们来选择的：

1. the choice of exponential family distribution
2. the choice of resonse function

对于指数族分布的选择主要受到 $Y$ 的特性的影响：

* 如果是类别：选择 Bernoulli 或者 Category
* 如果是计数：选择 Poisson
* 如果是时间间隔：选择 exponential 或者 gamma

对于 *response function*的选择：

* 通常在该函数上施加一些限制，这些限制通常反映的是条件期望的一些限制。距离说明，如果是 Bernolli 或者 Category，那么其条件期望应该在 $(0, 1)$区间内， 那么我们选择 *response function* 的时候也应该满足该性质。如果是 *gamma distribution*，那么 *response function* 的取值范围应该是 $(0, \infin)$



# chapter09：Completely Observed Graphical Models

> Completely Observed Graphical Models: 概率图中的每个节点都是观测的到的
>
> 对于有向图来说：可以直接分解为各个条件概率独立建模（广义线性模型）
>
> 对于无向图来说：因为有一个归一化项存在，所以有点难办。如何处理？
>
> Latent Variables: 图中无法观测到的变量称之为 latent variable. 对于存在latent variables 的图来说，likelihood 为 marginal probability, 需要通过 summing or integrating over latent variables. The log likelihood is the logarithm of this sum, and the logarithm is prevented from moving past the sum to act on the product of potentials. The parameter estimation problem does not decouple.

考虑一张图 $Z\rightarrow X_1, Z\rightarrow X_2$

如果 $Z, X_N$ 全都可观测, 就可以写成如下形式，就 decouple 了
$$
\begin{aligned}
l(\theta; x, z) &= \log p(x, z|\theta) \\\\
&=\log (p(z|\theta_z)p(x_1|z, \theta_1)p(x_2|z, \theta_2)) \\\\
&=\log p(z|\theta_z) + \log p(x_1|z, \theta_1) + \log p(x_2|z, \theta_2)
\end{aligned}
$$


如果$Z$ 为 latent variable. 会写成以下形式，就无法 decouple 了。

> 因为假设调整了变量 $\theta_1$ 就会影响所有其它的变量, 但是这个也是符合直觉的，我们关于 $z$ 的不确定性会反映到所有的 $X$ 上。*EM* 算法可以用来解决这个问题。*EM* algorithm allows us to solve the maximum likelihood problem for the latent variable by solving a sequence of maximum likelihook problem based on completely observed problems.

$$
\begin{aligned}
l(\theta; x) &= \log \sum_z p(x, z|\theta) \\\\
&=\log \sum_z p(z|\theta_z)p(x_1|z, \theta_1)p(x_2|z, \theta_2)
\end{aligned}
$$



# chapter10: mixture and conditional mixtures



# chapeter11: EM algorithm





# glossary

* `estimator`:参数的`estimator` 是关于观测数据`x`的一个函数。
*  
