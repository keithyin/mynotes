# chapter02

## 有向无环图

如何根据一个图对联合分布进行分解

* 根据图的**拓扑序**给每个顶点进行标号，然后按照以下方式进行分解就可以了。

 $p(x_1, x_2, ..., x_3) = \prod_{i=1}^n p(x_i| x_{\pi_i})$

* 其中：$x_{\pi_i}$ 表示节点 $x_i$ 的父节点



当判断 两个节点[集] 是否 [条件]独立的时候，就看两个节点[集] 之间是否有通路。**这个通路不考虑有向图边的方向**， 是否有通路是根据图的子结构的决定的，（线性结构，V-结构，$\Lambda$-结构）。不同的结构具有不同的判断路是不是通的方法。



## 无向图

使用最大割来定义联合概率的分解

$p(x) = \frac{1}{Z} \prod_{c \in \mathcal C} {\psi_{X_C}(x_C)}$



# chapter03



# chaper05

* 概率论：给定概率模型（`a graph and a set of local conditional probabilities or potentials`），如何根据概率模型进行概率计算。 `model -> data`
* 统计学：概率模型是未知的(`graph 已知，但是条件概率或者pontentials 未知。有时甚至 graph 都未知`)，需要根据观测的数据得到概率模型 （统计学从某种方面来说可以看做概率论的反操作）。 `data -> model`
  * 频率学派
  * 贝叶斯学派

统计学要解决的核心问题：1）密度估计，2）回归，3）分类



### 贝叶斯学派

> attempt to deny any fundamental distinction between probability theroy and statistics
>
> probability theory itself provides the capability for inverting relationships between uncertain quantities (**bayes rule**)
>
> bayesian statistics represents an attempt to treat all **statistical inference** as **probabilistic inference**.

**贝叶斯参数估计**
$$
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}
$$

* 可以看出，贝叶斯参数估计实际是产出了参数 $\theta$ 的一个概率分布，而不是一个值
* 如果我们想要获得一个**值**的话，需要一些额外的准则（`mean, mode`）

* 

### 频率学派

> The frequentist approch wishes to avoid the use of prior probabilities in statistics, and thus avoids the use of bayes rule for the purpose of assigning probabilities to parameters.
>
> 频率学派的目标是建立一个客观的统计理论，这样，不同的统计学家在同一个数据集上得到的结论是相同的。
>
> 贝叶斯学派就偏主观，因为对于先验分布的选择是主观的，这样不同的统计学家在同一个数据集上可能得到不同的结论。

已抛硬币的正面概率 $\theta$ 来讲：

* 频率学派：$\theta$ 是硬币的一个固有属性，给这个值一个概率是不 make sense 的。
* 贝叶斯学派：认同 $\theta$ 是硬币的一个固有属性，但同时也认为 $p(\theta)$是用来表示统计学家的不确定性，而不是用来表示硬币的物理性质。

### 一堆 estimator

关于 贝叶斯学派 和 频率学派如何看待 $p(x|\theta)$

* 一个直观的解释为： $p(x|\theta)$ ，assigning probability to $X$ for each fixed value of $\theta$
* 贝叶斯学派：$p(x|\theta)$ , 一个条件概率分布。（这种理解实际上是将 $\theta$ 看成了随机变量）
* 频率学派： $p(x|\theta)$, 将其看做一族与$\theta$相关的概率分布，（with no implication that we are conditioning on $\theta$）

当我们将 $p(x|\theta)$ 看做 固定了 $x$ 的，一个关于 $\theta$的函数，那么 $p(x|\theta)$就被称为 `likelihood function` 

给定一个 $\theta$ 值，我们可以使用似然函数进行评估。

**最大似然估计 (maximum likelihood estimate)**

> 遍历所有可能的 $\theta$ ，选择导致似然函数取得最大值的 $\theta$, 就构成了最大似然估计 estimator

$$
\hat \theta_{ML} = \arg\max_\theta p(x|\theta)
$$



**bayes estimate**
$$
\hat \theta_{Bayes} = \int \theta p(\theta|x)d\theta
$$
**maximum a posteriori estimate**
$$
\begin{aligned}
\hat \theta_{MAP} &= \arg\max_{\theta} p(\theta|x)\\
&=\arg\max_{\theta}p(x|\theta)p(\theta)
\end{aligned}
$$

* 从该公式可以看出，如果先验概率取 $p(\theta)$ 为 `uniform distribution` 的话，最大后验估计退化为最大似然估计
* 当先验概率$p(\theta)$ 不取 `uniforma distribution`的话，可以将最大后验估计看做为 `penalized likelihood`. 带惩罚项的最大似然估计常被频率派统计学家用在样本量很少的场景中。

$$
\hat \theta_{MAP} = \arg\max_{\theta} \{\log p(x|\theta) + \log p(\theta)\}
$$

> 从 MAP 上似乎难以看出来贝叶斯估计与频率估计区别，因为 $p(\theta)$ 既可以看做先验，又可以看做 panalty



### prediction

从 `prediction` 任务上来看贝叶斯学派与频率学派的区别

**贝叶斯学派**
$$
\begin{aligned}
p(x_{new}|x) &= \int p(x_{new}, \theta|x) d\theta\\
&=\int p(x_{new}|\theta, x)p(\theta|x)d\theta\\
&=\int p(x_{new}|\theta)p(\theta|x) d\theta
\end{aligned}
$$

* 由于将 $\theta$ 看做一个随机变量，所以这是一个积分。贝叶斯学派认为，相比于只用一个点进行估计，积分提供了一个更好的估计。

**频率学派**

$p(x_{new}|\hat\theta_{MAP}), p(x_{new}|\hat\theta_{ML})$

* 不认为 $\theta$ 是随机变量



### 密度估计

> 我们有随机变量 $X$ 的一堆观测值，我们希望通过这些观测值归纳出 $X$ 的概率密度。这就是密度估计

得到了 $X$ 的概率模型之后，我们可以通过该模型来判断一个 $X$ 的观测值是否 `typical` . 这个判断可以用在多个任务上，比如：`fault detection, outlier detection and clustering` 。密度估计也可以用在 *dimensionality reductions* 问题上。回归和分类实际也是在进行密度估计（条件概率密度估计）

**一元高斯密度估计**

> 这里主要关注的是如何使用概率图模型来表示统计问题。实际就是，关于随机变量 $X$ 的 N 个观测数据实际就是概率图中的 $N$ 个 `node`. 一般用 $\{X_1, X_2, .., X_N\}$ 表示。



### 回归



### 分类





# chapter06





# gloassary

* `estimator`:参数的`estimator` 是关于观测数据`x`的一个函数。
*  
