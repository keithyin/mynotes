# 机器学习概览



## 二分类问题

**精确率**

算法认为正类的样本中，有多少是正确的。
$$
P=\frac{TP}{TP+FP}
$$
**召回率**

算法挑出 正确的正例样本的比率。
$$
R=\frac{TP}{TP+FN}
$$
**准确率**
$$
Acc = \frac{TP+TN}{Num\_Samples}
$$
**F1**
$$
\frac{2}{F} = \frac{1}{P}+\frac{1}{R}
$$


## 朴素贝叶斯

生成模型，学习 $P(Y,X)$

**条件独立性假设**
$$
P(x_1,x_2,...,x_m|y) = \prod_i^m P(x_i|y)
$$
**模型的参数是**
$$
P(X_i=x_j|Y=y_k), P(Y=y_k)
$$
**模型训练**

* 使用 最大似然，或者 最大后验概率





## 决策树

> 树结构

* 如何构建决策树
* 如何剪枝

**样本**

$\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ...\}$



**如何选择特征**

* 信息增益: 
* 信息增益比
* Gini 指数



**基本概念**

* 数据集的 熵： 即 label 作为随机变量的熵。和特征并无关系。
* ​



**ID3 算法**

> 使用信息增益来作为特征选择的标准，特征值的个数即是当前节点的 子节点个数。 一个特征只使用一次。
>
> 离散特征，离散 label

$$
\begin{aligned}
g(D,A) &= H(D) - H(D|A) \\
H(D|A) &= \sum_{i=1}^n\frac{|D_i|}{|D|} H(D_i)
\end{aligned}
$$

* $D(D)$ ：当前数据集的熵
* $n$ ：表示当前特征 $A$ 的取值个数
* $|D_i|$ ：表示 数据集 $D$ 中 , 特征 $A=A_i$ 的样本个数。

ID3 算法就是通过 信息增益来选择 但前要用哪个特征进行划分。



**C4.5 算法**

> 和ID3不同的是，C4.5 使用 信息增益比作为特征选择的依据
>
> 离散特征。离散 label

$$
\begin{aligned}
g_R(D,A) &= \frac{g(D,A)}{H_A(D)} \\
H_A(D) &= -\sum_{i=1}^n\frac{|D_i|}{|D|}\log(\frac{|D_i|}{|D|})
\end{aligned}
$$

$H_A(D)$ 表示 特征 $A$ 的熵。



**为什么信息增益率比信息增益要好**

使用信息增益，存在优先选择 **取值较多的特征** 的问题。取值较多的特征虽然能够很好的划分数据集，但是可能会导致 模型的复杂度增高。容易导致过拟合的问题。 其实信息增益率 也是一个控制模型复杂度的方法。



**CART (classification and regression tree)**

> 分类 和 回归树， 不仅可以分类，而且可以回归。
>
> CART 假设决策树是 二叉树。
>
> 特征可连续，可离散，连续 label（回归嘛）。
>
> 对 分类树使用 Gini 指数， 对回归树使用 平方误差最小。



<font style="color:tomato">回归树：一个回归树对应着输入空间的一个划分以及在划分单元上的输出值。假设已经将输入空间划分成 $M$ 个单元，且每个单元 $R_m$ 上都有一个输出值 $c_m$ ，则回归树模型可以表示为</font>
$$
f(x) = \sum_{m=1}^M c_mI(x\in R_m)
$$
如何对输入空间进行划分，采用启发式的方法，选择第 $j$ 个变量 $x_j$ 和它取的值 $s$ 作为切分变量和切分点，并将其划分成两个区域： $R_1(j,s) = \{x|x_j\le s\}$ 和 $R_1(j,s) = \{x|x_j\gt s\}$ 。然后求解：
$$
\min_{j,s} \Biggr[\min_{c_1}\sum_{x^{(i)}\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2}\sum_{x^{(i)}\in R_2(j,s)}(y_i-c_2)^2 \Biggr]
$$

* 遍历 $j$
  * 扫描切分点 $s$
    * 求 $c_1$ 和 $c_2$ 



<font style="color:tomato"> 分类树：使用 Gini index 选择最优特征，同时决定该特征的值为 最优二值切分点。 </font>

> 特征离散， label 离散。

**基尼指数：** 分类问题中，假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p_k$ ，则概率分布的基尼指数定义为： 
$$
\text{Gini}(p) = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2
$$
对于给定的样本集合，Gini-index 为：Gini 指数越小，说明选择的特征越好。
$$
\text{Gini}(D) = 1-\sum_{k=1}^K\Bigr(\frac{|D_k|}{|D|}\Bigr)
$$

* $|D_k|$ : 数据集中，第  $k$  类的样本个数。



在特征 $A$ 下，集合 $D$ 的基尼指数定义为
$$
\text{Gini}(D,A) = \frac{|D_1|}{|D|}\text{Gini}(D_1) + \frac{|D_2|}{|D|}\text{Gini}(D_2)
$$




